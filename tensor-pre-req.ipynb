{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensor pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original: https://github.com/peluche/pre-arena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch as t\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "import gensim.downloader\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.manual_seed(0xdeadbeef)\n",
    "\n",
    "class CFG:\n",
    "    schools = 5\n",
    "    classes = 3\n",
    "    students = 20\n",
    "    exams = 10\n",
    "\n",
    "data = t.randint(0, 100, (CFG.schools, CFG.classes, CFG.students, CFG.exams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(frames, figsize=(6, 6), display_inline=True, filename=None):\n",
    "  def get_frame(ax):\n",
    "    def f(d):\n",
    "      ax.clear()\n",
    "      ax.axis('off')\n",
    "      ax.margins(0)\n",
    "      ax.imshow(frames[d], cmap='binary')\n",
    "    return f\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=figsize)\n",
    "  fig.tight_layout()\n",
    "  ani = FuncAnimation(fig, get_frame(ax), frames=len(frames), interval=50, repeat=False)\n",
    "  plt.close()\n",
    "  if display_inline: display(HTML(ani.to_jshtml())) # display inline\n",
    "  if filename is not None: ani.save(filename, fps=20) # save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.manual_seed(0xdeadbeef)\n",
    "x = t.randn(20).view(-1, 2)\n",
    "normalized_x = (x - x.mean(0, keepdim=True))\n",
    "colors = t.arange(len(normalized_x))\n",
    "\n",
    "def get_best_angle(normalized_x):\n",
    "    # /!\\ this is for demo purpose only\n",
    "    mini = float('inf')\n",
    "    best_angle = 0\n",
    "    # iterate over all (~not really but shush~)\n",
    "    for i in range(0, 180):\n",
    "        sum_of_squared_distances = 0\n",
    "        angle = math.radians(i)\n",
    "        slope = t.tensor([math.cos(angle), math.sin(angle)])\n",
    "        for point in normalized_x:\n",
    "            projection = (point.dot(slope) / slope.dot(slope)) * slope\n",
    "            squared_distance = (point - projection).pow(2).sum()\n",
    "            sum_of_squared_distances += squared_distance\n",
    "        if sum_of_squared_distances < mini:\n",
    "            mini = sum_of_squared_distances\n",
    "            best_angle = i\n",
    "    return best_angle\n",
    "\n",
    "def plot_line(plt, angle):\n",
    "    ar = math.radians(angle)\n",
    "    v = t.tensor([math.cos(ar), math.sin(ar)])\n",
    "    x_min, x_max, y_min, y_max = plt.axis()\n",
    "    diag = t.tensor([x_max - x_min, y_max - y_min]).norm()\n",
    "    v *= diag / 2.\n",
    "    plt.plot([-v[0], v[0]], [-v[1], v[1]], linestyle='--', color='red', label=f'Angle: {angle}°')\n",
    "\n",
    "def rotate(data, angle):\n",
    "    angle = math.radians(angle)\n",
    "    rotation_matrix = t.tensor([[math.cos(angle), -math.sin(angle)], [math.sin(angle), math.cos(angle)]])\n",
    "    rotated = t.zeros_like(data)\n",
    "    for i, point in enumerate(data):\n",
    "        rotated[i] = rotation_matrix.T @ point\n",
    "    return rotated\n",
    "\n",
    "def plot_step(ax, data, angle):\n",
    "    ax.scatter(data[:, 0], data[:, 1], c=colors, cmap='viridis')\n",
    "    plot_line(ax, angle)\n",
    "\n",
    "def get_scale(data, angle, delta=0.1):\n",
    "    xs = [*data[:, 0].tolist()]\n",
    "    ys = [*data[:, 1].tolist()]\n",
    "    for i in range(angle + 1):\n",
    "        rotated = rotate(data, angle)\n",
    "        xs.extend(rotated[:, 0].tolist())\n",
    "        ys.extend(rotated[:, 1].tolist())\n",
    "    return min(xs) - delta, max(xs) + delta, min(ys) - delta, max(ys) + delta\n",
    "\n",
    "def get_frame(ax, data, angle):\n",
    "    min_x, max_x, min_y, max_y = get_scale(data, angle)    \n",
    "    def f(step):\n",
    "        ax.clear()\n",
    "        ax.set_xlim(min_x, max_x)\n",
    "        ax.set_ylim(min_y, max_y)\n",
    "        rotated = rotate(data, step)\n",
    "        plot_step(ax, rotated, angle - step)\n",
    "    return f\n",
    "\n",
    "def animate_pca(data, angle, figsize=(5, 5)):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ani = FuncAnimation(fig, get_frame(ax, data, angle), frames=angle + 1, interval=50, repeat=False)\n",
    "    plt.close()\n",
    "    return ani\n",
    "\n",
    "def pca_basis_change():\n",
    "    best_angle = get_best_angle(normalized_x)\n",
    "    ani = animate_pca(normalized_x, best_angle)\n",
    "    display(HTML(ani.to_jshtml())) # display in notebook\n",
    "    # ani.save('imgs/pca_rotation.gif', fps=20) # save to disk\n",
    "\n",
    "def plot_step_collapse(ax, data, scale):\n",
    "    ax.scatter(data[:, 0], data[:, 1] * scale, c=colors, cmap='viridis')\n",
    "    plot_line(ax, 0)\n",
    "\n",
    "def get_frame_collapse(ax, data, steps):\n",
    "    min_x, max_x, min_y, max_y = get_scale(data, 0)\n",
    "    def f(step):\n",
    "        ax.clear()\n",
    "        ax.set_xlim(min_x, max_x)\n",
    "        ax.set_ylim(min_y, max_y)\n",
    "        plot_step_collapse(ax, data, (steps - step) / steps)\n",
    "    return f\n",
    "\n",
    "def animate_pca_collapse(data, figsize=(5, 5)):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    steps = 30\n",
    "    ani = FuncAnimation(fig, get_frame_collapse(ax, data, steps), frames=steps + 1, interval=50, repeat=False)\n",
    "    plt.close()\n",
    "    return ani\n",
    "\n",
    "def pca_collapse():\n",
    "    best_angle = get_best_angle(normalized_x)\n",
    "    rotated = rotate(normalized_x, best_angle)\n",
    "    ani = animate_pca_collapse(rotated)\n",
    "    display(HTML(ani.to_jshtml())) # display in notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answers /!\\ SPOILER INSIDE /!\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tensor manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the highest score in the district\n",
    "def _district_highest_score(data):\n",
    "    return data.max()\n",
    "def test_district_highest_score(x):\n",
    "    assert x.allclose(_district_highest_score(data))\n",
    "\n",
    "# find the best score for each exam in each class in each school\n",
    "def _best_score_per_exam(data):\n",
    "    return data.max(dim=2).values\n",
    "def test_best_score_per_exam(x):\n",
    "    assert x.allclose(_best_score_per_exam(data))\n",
    "\n",
    "# what class has the best average score in each school\n",
    "def _best_class(data):\n",
    "    return data.mean(dim=(2, 3), dtype=t.float32).argmax(dim=1)\n",
    "def test_best_class(x):\n",
    "    assert x.allclose(_best_class(data))\n",
    "\n",
    "# compute how many exams each student has failed (score < 50)\n",
    "def _failed_exams(data):\n",
    "    return (data < 50).sum(dim=3)\n",
    "def test_failed_exams(x):\n",
    "    assert x.allclose(_failed_exams(data))\n",
    "\n",
    "# what was the hardest exam accross the district\n",
    "def _hardest_exam(data):\n",
    "    return data.sum(dim=(0, 1, 2)).argmin()\n",
    "def test_hardest_exam(x):\n",
    "    assert x.allclose(_hardest_exam(data))\n",
    "\n",
    "# how many students will graduate (GPA >= 50)\n",
    "def _how_many_graduate(data):\n",
    "    return (data.float().mean(dim=3) > 50).sum()\n",
    "def test_how_many_graduate(x):\n",
    "    assert x.allclose(_how_many_graduate(data))\n",
    "\n",
    "# how many more students will graduate if the hardest exam is removed\n",
    "def _how_many_graduate_without_hardest_exam(data):\n",
    "    copy = data.clone()\n",
    "    copy[:, :, :, _hardest_exam(data)] = 50\n",
    "    return _how_many_graduate(copy) - _how_many_graduate(data)\n",
    "def _how_many_graduate_without_hardest_exam2(data):\n",
    "    hardest_exam = _hardest_exam(data)\n",
    "    data_without_hardest_exam = t.cat((data[..., 0:hardest_exam], data[..., hardest_exam+1:]), dim=3)\n",
    "    return _how_many_graduate(data_without_hardest_exam) - _how_many_graduate(data)\n",
    "def test_how_many_graduate_without_hardest_exam(x):\n",
    "    assert x.allclose(_how_many_graduate_without_hardest_exam(data))\n",
    "    assert x.allclose(_how_many_graduate_without_hardest_exam2(data))\n",
    "\n",
    "# what score did the worst student (lowest GPA) got on the their best exam\n",
    "def _worst_student_best_exam(data):\n",
    "    lowest_gpa = data.sum(dim=3).min()\n",
    "    idx = data.sum(dim=3) == lowest_gpa\n",
    "    return data[idx].max()\n",
    "def test_worst_student_best_exam(x):\n",
    "    assert x.allclose(_worst_student_best_exam(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### game of life"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _count_neighbors(world):\n",
    "    kernel = t.ones(1, 1, 3, 3)\n",
    "    kernel[0, 0, 1, 1] = 0\n",
    "    return F.conv2d(world[None, None], kernel, padding=1)[0, 0]\n",
    "\n",
    "def test_count_neighbors(f, world):\n",
    "    assert f(world).allclose(_count_neighbors(world))\n",
    "\n",
    "def _next_step(world):\n",
    "    neighbors = _count_neighbors(world)\n",
    "    next_generation = (neighbors == 3) | ((world == 1) & (neighbors == 2))\n",
    "    return next_generation.float()\n",
    "\n",
    "def test_next_step(f, world):\n",
    "    assert f(world).allclose(_next_step(world))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### embeddings / word2vec / PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the similarity between two words using cosine similarity https://en.wikipedia.org/wiki/Cosine_similarity\n",
    "def _cosine_similarity(word1, word2, word2vec):\n",
    "    e1 = t.tensor(word2vec[word1])\n",
    "    e2 = t.tensor(word2vec[word2])\n",
    "    return t.dot(e1, e2) / (t.norm(e1) * t.norm(e2))\n",
    "def test_cosine_similarity(f, word1, word2, word2vec):\n",
    "    assert f(word1, word2).allclose(_cosine_similarity(word1, word2, word2vec))\n",
    "\n",
    "# create a 2d tensor of shape [words, embedding_size]\n",
    "def _words_to_embeddings(words, word2vec):\n",
    "    embeddings = [t.tensor(word2vec[word]) for word in words]\n",
    "    return t.stack(embeddings)\n",
    "def test_words_to_embeddings(f, words, word2vec):\n",
    "    assert f(words).allclose(_words_to_embeddings(words, word2vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working with a 4d tensor `data` representing each schools in a district of shape `[school, class, student, exam]`\n",
    "\n",
    "For example:\n",
    "`data[0, 5, 2, 1]`\n",
    "\n",
    "would tell us what score the 3rd student in the 6th class of the first school scored on their second exam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the highest score in the district\n",
    "# <STUDENT JOB>\n",
    "answer = \n",
    "\n",
    "test_district_highest_score(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best score for each exam in each class in each school\n",
    "# <STUDENT JOB>\n",
    "answer = \n",
    "\n",
    "test_best_score_per_exam(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what class has the best average score in each school\n",
    "# <STUDENT JOB>\n",
    "answer = \n",
    "\n",
    "test_best_class(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute how many exams each student has failed (score < 50)\n",
    "# <STUDENT JOB>\n",
    "answer = \n",
    "\n",
    "test_failed_exams(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what was the hardest exam accross the district\n",
    "# <STUDENT JOB>\n",
    "answer = \n",
    "\n",
    "test_hardest_exam(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many students will graduate (GPA >= 50)\n",
    "# <STUDENT JOB>\n",
    "answer = \n",
    "\n",
    "test_how_many_graduate(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many more students will graduate if the hardest exam is removed\n",
    "# <STUDENT JOB>\n",
    "answer = \n",
    "\n",
    "test_how_many_graduate_without_hardest_exam(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what score did the worst student (lowest GPA) got on their best exam\n",
    "# <STUDENT JOB>\n",
    "answer = \n",
    "\n",
    "test_worst_student_best_exam(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## game of life"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life\n",
    "\n",
    "we can represent the world as a 2d tensor of shape `[height, width]` where `0.` means dead, and `1.` means alive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = t.tensor([\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0]], dtype=t.float32)\n",
    "\n",
    "world.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "game of life is a celullar automata obeying the following rules:\n",
    "- Any live cell with fewer than two live neighbors dies, as if by underpopulation.\n",
    "- Any live cell with two or three live neighbors lives on to the next generation.\n",
    "- Any live cell with more than three live neighbors dies, as if by overpopulation.\n",
    "- Any dead cell with exactly three live neighbors becomes a live cell, as if by reproduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find how many neighbors each cell has\n",
    "def count_neighbors(world):\n",
    "    # <STUDENT JOB>\n",
    "    pass\n",
    "\n",
    "test_count_neighbors(count_neighbors, world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: boolean operations between tensors have lower priorities so they require parenthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a new world for next step according to the rules of game of life\n",
    "def next_step(world):\n",
    "    world = world.clone()\n",
    "    # <STUDENT JOB>\n",
    "    pass\n",
    "\n",
    "test_next_step(next_step, world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(world, epochs=10):\n",
    "    frames = [world]\n",
    "    for _ in range(epochs):\n",
    "        world = next_step(world)\n",
    "        frames.append(world.clone())\n",
    "    draw(frames)\n",
    "\n",
    "play(world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embeddings / word2vec / PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell and go grab a drink "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.downloader.load('word2vec-google-news-300') # 1.6 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec is an embedding table. It associates words to some vector of float numbers. It has been trained by training a network to predict the middle word in sentences from google news.\n",
    "\n",
    "e.g. \"There riots in _____ continue, french people are complaining about retirements\" → \"Paris\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = t.tensor(word2vec['Paris'])\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the similarity between two words using cosine similarity https://en.wikipedia.org/wiki/Cosine_similarity\n",
    "def cosine_similarity(word1, word2):\n",
    "    # <STUDENT JOB>\n",
    "    pass\n",
    "\n",
    "test_cosine_similarity(cosine_similarity, 'Paris', 'France', word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that similar words have bigger cosine similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in ['Paris', 'France', 'Germany', 'tomato']:\n",
    "    print(f'cosine similarity between \"Paris\" and \"{word}\": {cosine_similarity(\"Paris\", word):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One fun application is to use these embedding (vector representation of a word) to do word arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = word2vec['Paris'] - word2vec['France'] + word2vec['Germany']\n",
    "word2vec.similar_by_vector(e, topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = word2vec['king'] - word2vec['man'] + word2vec['woman']\n",
    "word2vec.similar_by_vector(e, topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = word2vec['Italy'] - word2vec['Rome'] + word2vec['Tokyo']\n",
    "word2vec.similar_by_vector(e, topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the embeddings in 2d using PCA https://en.wikipedia.org/wiki/Principal_component_analysis\n",
    "\n",
    "PCA lets us reduce the dimensionality of the data by changing the basis vector for our vector space and throwing away the basis with the least significances. It's a lossy way to compress a vector into a smaller vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First by chosing the line that pass through the origin and minimize the distance to all points, and rotating around it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_basis_change()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then collapsing the points into this line (throwing away the `y` axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_collapse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can compute the PCA for a set of vectors using https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tensor of shape [words, embedding_size]\n",
    "def words_to_embeddings(words):\n",
    "    # <STUDENT JOB>\n",
    "    pass\n",
    "\n",
    "words = ['Paris', 'Rome', 'Tokyo', 'tomato', 'banana', 'apple', 'king', 'queen']\n",
    "test_words_to_embeddings(words_to_embeddings, words, word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is hard to visualize embeddings with 300 dimensions, but we can use 2D PCA to visualize the embeddings of the words in 2D and build some intuitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(words_to_embeddings(words))\n",
    "plt.scatter(principal_components[:, 0], principal_components[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, (principal_components[i, 0], principal_components[i, 1]))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
